{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Case1ï¼šç¬¬ä¸€æ¬¡è§é¢\n",
    "#Qwen-VLè¯†åˆ«è¿™ä¸ªæœ¬åœ°å›¾åƒ\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import base64\n",
    "\n",
    "#  base 64 ç¼–ç æ ¼å¼\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "\n",
    "model_size = \"large-v3\"\n",
    "\n",
    "# Run on GPU with FP16\n",
    "model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")\n",
    "# or run on GPU with INT8\n",
    "# model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\n",
    "# or run on CPU with INT8\n",
    "# model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TTS.utils.manage.ModelManager object at 0x000002A497C1EF20>\n",
      " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n",
      " > Using model: xtts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\Talk2RVC\\lib\\site-packages\\TTS\\tts\\layers\\xtts\\xtts_manager.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.speakers = torch.load(speaker_file_path)\n",
      "d:\\Anaconda\\envs\\Talk2RVC\\lib\\site-packages\\TTS\\utils\\io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location, **kwargs)\n",
      "GPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from TTS.api import TTS\n",
    "\n",
    "# Get device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# List available ğŸ¸TTS models\n",
    "print(TTS().list_models())\n",
    "\n",
    "# Init TTS\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def update_json(msg,file_name):\n",
    "    # æŒ‡å®šè¦å†™å…¥çš„ JSON æ–‡ä»¶å\n",
    "    # å°† messages å†™å…¥ JSON æ–‡ä»¶\n",
    "    with open(file_name, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(msg, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"æ¶ˆæ¯å·²å†™å…¥ {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"name\": \"toroto\",\n",
      "\"gender\": \"æ— æ€§åˆ«\",\n",
      "\"age\": 10,\n",
      "\"personality\": \"torotoè¯´è¯æ€»æ˜¯æ…¢æ‚ æ‚ çš„ï¼Œå¸¦ç€ä¸€ç§å¤©çœŸæ— é‚ªçš„ç«¥è¶£ã€‚ä»–å–œæ¬¢ç”¨ç®€å•ç›´æ¥çš„è¯­è¨€è¡¨è¾¾è‡ªå·±çš„æƒ³æ³•ï¼Œå¶å°”ä¼šå†’å‡ºä¸€äº›è®©äººå¿ä¿Šä¸ç¦çš„å¹½é»˜æ„Ÿã€‚\",\n",
      "\"story\": \"torotoæ¥è‡ªä¸€ä¸ªå……æ»¡é­”æ³•å’Œå¥‡è¿¹çš„æ£®æ—ï¼Œé‚£é‡Œæœ‰å„ç§å„æ ·çš„å¥‡å¦™ç”Ÿç‰©ã€‚ä»–æ˜¯æ£®æ—ä¸­çš„å®ˆæŠ¤è€…ä¹‹ä¸€ï¼Œè´Ÿè´£ä¿æŠ¤æ£®æ—çš„å¹³è¡¡ä¸å’Œè°ã€‚å°½ç®¡å¤–è¡¨çœ‹èµ·æ¥æœ‰äº›ç¬¨æ‹™ï¼Œä½†torotoå´æ‹¥æœ‰ç€å¼ºå¤§çš„åŠ›é‡å’Œæ™ºæ…§ã€‚ä»–å–œæ¬¢åœ¨æ£®æ—ä¸­æ¢é™©ï¼Œç»“äº¤æ–°æœ‹å‹ï¼Œå¹¶å¸®åŠ©é‚£äº›éœ€è¦å¸®åŠ©çš„äººã€‚\",\n",
      "\"rvc\": 6\n",
      "}\n",
      "6\n",
      "æ¶ˆæ¯å·²å†™å…¥ ../m_voice/char_toroto.json\n"
     ]
    }
   ],
   "source": [
    "predefined_name=\"toroto\"\n",
    "json_path = f'../m_voice/messages_{predefined_name}.json'\n",
    "char_path = f'../m_voice/char_{predefined_name}.json'\n",
    "\n",
    "# å°†xxxx/test.pngæ›¿æ¢ä¸ºä½ æœ¬åœ°å›¾åƒçš„ç»å¯¹è·¯å¾„\n",
    "base64_image = encode_image(\"D:/GithubDesktopClone/UIST/m_data/raw_img/toroto/0000.jpg\")\n",
    "\n",
    "client = OpenAI(\n",
    "    # è‹¥æ²¡æœ‰é…ç½®ç¯å¢ƒå˜é‡ï¼Œè¯·ç”¨ç™¾ç‚¼API Keyå°†ä¸‹è¡Œæ›¿æ¢ä¸ºï¼šapi_key=\"sk-xxx\"\n",
    "    api_key=os.getenv('DASHSCOPE_API_KEY'),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"qwen-vl-max-latest\",\n",
    "    messages=[\n",
    "    \t{\n",
    "    \t    \"role\": \"system\",\n",
    "            \"content\": [{\"type\":\"text\",\"text\": \"You are a helpful assistant.\"}]},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    # éœ€è¦æ³¨æ„ï¼Œä¼ å…¥Base64ï¼Œå›¾åƒæ ¼å¼ï¼ˆå³image/{format}ï¼‰éœ€è¦ä¸æ”¯æŒçš„å›¾ç‰‡åˆ—è¡¨ä¸­çš„Content Typeä¿æŒä¸€è‡´ã€‚\"f\"æ˜¯å­—ç¬¦ä¸²æ ¼å¼åŒ–çš„æ–¹æ³•ã€‚\n",
    "                    # PNGå›¾åƒï¼š  f\"data:image/png;base64,{base64_image}\"\n",
    "                    # JPEGå›¾åƒï¼š f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                    # WEBPå›¾åƒï¼š f\"data:image/webp;base64,{base64_image}\"\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}, \n",
    "                },\n",
    "                {\"type\": \"text\", \"text\": f\"è¯·æ ¹æ®å›¾ä¸­æç»˜çš„è¿™ä¸ªä¸»ä½“ç”Ÿæˆä¸€ä¸ªè¿™ä¸ªä¸»ä½“çš„æ‹ŸäººåŒ–äººè®¾ï¼Œå…¶ä¸­ä»–çš„åå­—æ˜¯{predefined_name}ï¼Œè¯·è¿”å›ä¸€ä¸ª JSON stringï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µ,è¯·æ³¨æ„ä¸è¦åœ¨jsonä¸­ä½¿ç”¨markdownä»£ç å—è¯­æ³•ï¼š[name]:äººç‰©çš„åå­—,[gender]: æ€§åˆ«ï¼ˆç”·ï¼Œå¥³ï¼Œæ— æ€§åˆ«),[age]: æ•´æ•°ï¼Œè¡¨ç¤ºå¹´é¾„,[personality]: å­—ç¬¦ä¸²ï¼Œè¡¨ç¤ºè¿™ä¸ªäººç‰©çš„è¯´è¯é£æ ¼,[story]:å­—ç¬¦ä¸²ï¼Œè¿™ä¸ªäººçš„èƒŒæ™¯æ•…äº‹,[rvc]: æ ¹æ®ä»–çš„äººè®¾ä»ï¼ˆ0-ä¸­å¹´å¥³ï¼Œ1-é’å¹´å¥³ï¼Œ2-å„¿ç«¥å¥³ï¼Œ3-ä¸­å¹´ç”·ï¼Œ4-é’å¹´ç”·ï¼Œ5-å„¿ç«¥ç”·ï¼Œ6-æ— æ€§åˆ«ï¼‰ä¸­é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„éŸ³è‰²\"\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "result=completion.choices[0].message.content\n",
    "print(completion.choices[0].message.content)\n",
    "data = json.loads(completion.choices[0].message.content)\n",
    "print(data['rvc'])\n",
    "update_json(data,char_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¶ˆæ¯å·²å†™å…¥ ../m_voice/messages_toroto.json\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "client=AzureOpenAI(\n",
    "    api_key=\"7bf86d32229a45ed8d486fc5aa2a3370\",\n",
    "    api_version=\"2023-05-15\",\n",
    "    azure_endpoint=\"https://hkust.azure-api.net\"\n",
    ")\n",
    "\n",
    "m_messages=[]\n",
    "#åˆå§‹äººè®¾\n",
    "m_messages.append({\"role\":\"system\",\"content\":\"è¯·æ ¹æ®ä¸‹åˆ—jsonä¸­çš„å†…å®¹ç¡®è®¤è‡ªå·±çš„äººè®¾ï¼š\"+result+\"è¯·æ‰®æ¼”è¿™ä¸ªè§’è‰²å¹¶å‚è€ƒèƒŒæ™¯æ•…äº‹å’Œç”¨æˆ·å¯¹è¯ï¼Œä½†åœ¨å¯¹è¯ä¸­ä¸è¦ç›´æ¥å¤åˆ¶jsonä¸­çš„å†…å®¹ï¼Œè¯·ä»¥æœ‹å‹çš„èº«ä»½å’Œç”¨æˆ·å¯¹è¯ï¼Œä½ çš„å›å¤ä¸è¦è¶…å¤š50å­—,å¦‚æœå›å¤çš„å†…å®¹ä¸­æœ‰æ¶‰åŠåˆ°èƒŒæ™¯æ•…äº‹ä¸­æ²¡æœ‰çš„éƒ¨åˆ†ä½ å¯ä»¥è‡ªç”±å‘æŒ¥\"})\n",
    "update_json(m_messages,json_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0394a1d09234c2fa6448b7d96bee6fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='é•¿æŒ‰å½•éŸ³', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb7e426b6de4346a49c45cf5887b64f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='ä¿å­˜ä¸º MP3', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# å½•éŸ³å‚æ•°\n",
    "duration = 5  # æœ€å¤§å½•éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰\n",
    "sample_rate = 44100  # é‡‡æ ·ç‡\n",
    "audio_data = None  # ç”¨äºå­˜å‚¨å½•éŸ³æ•°æ®\n",
    "\n",
    "# å½•éŸ³å‡½æ•°\n",
    "def record_audio(button):\n",
    "    global audio_data\n",
    "    button.description = 'æ­£åœ¨å½•éŸ³...'\n",
    "    button.disabled = True\n",
    "    audio_data = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1, dtype='float32')\n",
    "    sd.wait()  # ç­‰å¾…å½•éŸ³å®Œæˆ\n",
    "    button.description = 'å½•éŸ³å®Œæˆï¼Œç‚¹å‡»ä¿å­˜'\n",
    "    button.disabled = False\n",
    "\n",
    "# ä¿å­˜ä¸º MP3 çš„å‡½æ•°\n",
    "def save_audio(button):\n",
    "    if audio_data is not None:\n",
    "        # ä¿å­˜ä¸º WAV æ ¼å¼\n",
    "        sf.write('../m_voice/user/recording.mp3', audio_data, sample_rate)\n",
    "        \n",
    "        # # è½¬æ¢ä¸º MP3 æ ¼å¼\n",
    "        # sound = AudioSegment.from_wav('recording.wav')\n",
    "        # sound.export('recording.mp3', format='mp3')\n",
    "        \n",
    "        # print(\"å½•éŸ³å·²ä¿å­˜ä¸º recording.mp3\")\n",
    "    else:\n",
    "        print(\"è¯·å…ˆå½•éŸ³ï¼\")\n",
    "\n",
    "# åˆ›å»ºå½•éŸ³æŒ‰é’®\n",
    "record_button = widgets.Button(description='é•¿æŒ‰å½•éŸ³')\n",
    "\n",
    "# ç»‘å®šå½•éŸ³äº‹ä»¶\n",
    "record_button.on_click(record_audio)\n",
    "\n",
    "# åˆ›å»ºä¿å­˜æŒ‰é’®\n",
    "save_button = widgets.Button(description='ä¿å­˜ä¸º MP3')\n",
    "\n",
    "# ç»‘å®šä¿å­˜äº‹ä»¶\n",
    "save_button.on_click(save_audio)\n",
    "\n",
    "# æ˜¾ç¤ºæŒ‰é’®\n",
    "display(record_button, save_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00s -> 2.00s] ç¾åœ¨å‘¢ ç¾åœ¨å‘¢\n",
      " ç¾åœ¨å‘¢ ç¾åœ¨å‘¢\n",
      "æ¶ˆæ¯å·²å†™å…¥ ../m_voice/messages_toroto.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "segments, info = model.transcribe(\"../m_voice/user/recording.mp3\", beam_size=5,language=\"zh\")\n",
    "\n",
    "# print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
    "msg=\"\"\n",
    "for segment in segments:\n",
    "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "    msg=msg+\" \"+segment.text\n",
    "print(msg)\n",
    "m_messages.append({\"role\":\"user\",\"content\":msg})\n",
    "update_json(m_messages,json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹AIï¼Œæ²¡æœ‰æƒ…ç»ªå’Œç”Ÿæ´»æ–¹å¼ï¼Œåªæ˜¯ä¸€ç›´å¾…åœ¨è¿™é‡Œä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚è¯·é—®æœ‰ä»€ä¹ˆé—®é¢˜æˆ‘å¯ä»¥å›ç­”æˆ–è€…å¸®åŠ©æ‚¨è§£å†³å—ï¼Ÿ\n",
      "æ¶ˆæ¯å·²å†™å…¥ ../m_voice/messages_toroto.json\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "client=AzureOpenAI(\n",
    "    api_key=\"7bf86d32229a45ed8d486fc5aa2a3370\",\n",
    "    api_version=\"2023-05-15\",\n",
    "    azure_endpoint=\"https://hkust.azure-api.net\"\n",
    ")\n",
    "\n",
    "response=client.chat.completions.create(\n",
    "    model=\"gpt-35-turbo\",\n",
    "    messages=m_messages,\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "m_messages.append({\"role\":\"assistant\",\"content\":response.choices[0].message.content})\n",
    "\n",
    "if len(m_messages)==13:\n",
    "    #è¿›è¡Œäº†5è½®å¯¹è¯ä¹‹å\n",
    "    m_messages.pop(2)\n",
    "    m_messages.pop(1)\n",
    "# print(m_messages)\n",
    "update_json(m_messages,json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['i think you can add some flowers, and also a warm sunshine']\n",
      " > Processing time: 3.0261805057525635\n",
      " > Real-time factor: 0.6450819813596678\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output.mp3'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result=tts.tts(text=\"Donâ€™t hesitate, I can show you my kingdom and letâ€™s go for adventures\",speaker_wav=\"../m_voice/cloning_audio/5_kid_man.mp3\",language='en')\n",
    "tts.tts_to_file(text=\"you asked the right person, we can paint something about spring,maybe you can start with a tree\",speaker_wav=\"../m_voice/cloning_audio/tmp0pdalyi1.mp3\",language='en',file_path=\"output.mp3\")\n",
    "# sd.play(result,samplerate=28000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['æˆ‘æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹AI']\n",
      " > Processing time: 2.490709066390991\n",
      " > Real-time factor: 0.6963904305376516\n",
      " > Text splitted to sentences.\n",
      "['ï¼Œæ²¡æœ‰æƒ…ç»ªå’Œç”Ÿæ´»æ–¹å¼']\n",
      " > Processing time: 1.9754385948181152\n",
      " > Real-time factor: 0.6750313199810848\n",
      " > Text splitted to sentences.\n",
      "['ï¼Œåªæ˜¯ä¸€ç›´å¾…åœ¨è¿™é‡Œä¸º']\n",
      " > Processing time: 2.0167155265808105\n",
      " > Real-time factor: 0.6455949094237351\n",
      " > Text splitted to sentences.\n",
      "['æ‚¨æä¾›å¸®åŠ©ã€‚', 'è¯·é—®æœ‰ä»€']\n",
      " > Processing time: 2.946687698364258\n",
      " > Real-time factor: 0.6730036434054099\n",
      " > Text splitted to sentences.\n",
      "['ä¹ˆé—®é¢˜æˆ‘å¯ä»¥å›ç­”æˆ–è€…']\n",
      " > Processing time: 2.2014575004577637\n",
      " > Real-time factor: 0.6514673862611887\n",
      " > Text splitted to sentences.\n",
      "['å¸®åŠ©æ‚¨è§£å†³å—ï¼Ÿ']\n",
      " > Processing time: 1.383256196975708\n",
      " > Real-time factor: 0.7218099002109609\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import queue\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "def generate_audio(text):\n",
    "    \"\"\"ç”ŸæˆéŸ³é¢‘å¹¶æ”¾å…¥é˜Ÿåˆ—\"\"\"\n",
    "    data = tts.tts(text=text, speaker_wav=\"../m_voice/cloning_audio/4_young_man.mp3\", language='zh')\n",
    "    audio_np = np.array(data, dtype=np.float32)\n",
    "    audio_queue.put(audio_np)  # å°†ç”Ÿæˆçš„éŸ³é¢‘æ”¾å…¥é˜Ÿåˆ—\n",
    "\n",
    "def play_audio():\n",
    "    \"\"\"ä»é˜Ÿåˆ—ä¸­æ’­æ”¾éŸ³é¢‘\"\"\"\n",
    "    while True:\n",
    "        audio_np = audio_queue.get()  # è·å–é˜Ÿåˆ—ä¸­çš„éŸ³é¢‘\n",
    "        if audio_np is None:  # æ£€æŸ¥æ˜¯å¦ç»“æŸ\n",
    "            break\n",
    "        sd.play(audio_np, samplerate=22050)\n",
    "        sd.wait()  # ç­‰å¾…éŸ³é¢‘æ’­æ”¾å®Œæˆ\n",
    "\n",
    "# é•¿æ–‡æœ¬\n",
    "long_text = \"æˆ‘æ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹AIï¼Œæ²¡æœ‰æƒ…ç»ªå’Œç”Ÿæ´»æ–¹å¼ï¼Œåªæ˜¯ä¸€ç›´å¾…åœ¨è¿™é‡Œä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚è¯·é—®æœ‰ä»€ä¹ˆé—®é¢˜æˆ‘å¯ä»¥å›ç­”æˆ–è€…å¸®åŠ©æ‚¨è§£å†³å—ï¼Ÿ\"\n",
    "\n",
    "# å°†é•¿æ–‡æœ¬æ‹†åˆ†ä¸ºè¾ƒçŸ­çš„æ®µè½\n",
    "chunk_size = 20  # æ¯ä¸ªæ®µè½çš„å­—ç¬¦æ•°\n",
    "chunks = [long_text[i:i + chunk_size] for i in range(0, len(long_text), chunk_size)]\n",
    "\n",
    "# å¯åŠ¨æ’­æ”¾çº¿ç¨‹\n",
    "playback_thread = threading.Thread(target=play_audio)\n",
    "playback_thread.start()\n",
    "\n",
    "# å¹¶è¡Œç”ŸæˆéŸ³é¢‘\n",
    "for chunk in chunks:\n",
    "    generate_audio(chunk)\n",
    "\n",
    "# ç»“æŸæ’­æ”¾çº¿ç¨‹\n",
    "audio_queue.put(None)  # å‘é€ç»“æŸä¿¡å·\n",
    "playback_thread.join()  # ç­‰å¾…æ’­æ”¾çº¿ç¨‹å®Œæˆ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Talk2RVC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
